{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 1: Training a PINN on the Poisson Equation\n",
    "\n",
    "**Learning Objectives:**\n",
    "- Understand what Physics-Informed Neural Networks (PINNs) are\n",
    "- Learn how PINNs solve partial differential equations (PDEs)\n",
    "- Train a PINN to solve the 2D Poisson equation\n",
    "- Visualize and validate the solution\n",
    "- Extract activations for interpretability analysis\n",
    "\n",
    "**Estimated Time:** 30-40 minutes\n",
    "\n",
    "---\n",
    "\n",
    "## Table of Contents\n",
    "1. [Introduction to PINNs](#1-Introduction-to-PINNs)\n",
    "2. [The Poisson Equation](#2-The-Poisson-Equation)\n",
    "3. [Setup and Imports](#3-Setup-and-Imports)\n",
    "4. [Creating the PINN Model](#4-Creating-the-PINN-Model)\n",
    "5. [Defining the Problem](#5-Defining-the-Problem)\n",
    "6. [Training the PINN](#6-Training-the-PINN)\n",
    "7. [Visualizing Results](#7-Visualizing-Results)\n",
    "8. [Extracting Activations](#8-Extracting-Activations)\n",
    "9. [Summary and Next Steps](#9-Summary-and-Next-Steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction to PINNs\n",
    "\n",
    "### What are Physics-Informed Neural Networks?\n",
    "\n",
    "**Physics-Informed Neural Networks (PINNs)** are neural networks that learn to solve partial differential equations (PDEs) by incorporating the physics of the problem directly into the loss function.\n",
    "\n",
    "**Traditional approach:**\n",
    "- Discretize the domain (finite differences, finite elements)\n",
    "- Solve a large system of equations\n",
    "- Requires mesh generation, can be complex\n",
    "\n",
    "**PINN approach:**\n",
    "- Neural network approximates the solution: $u(x) \\approx NN(x)$\n",
    "- Loss function combines:\n",
    "  - **PDE residual**: How well does the network satisfy the PDE?\n",
    "  - **Boundary conditions**: Does it match boundary values?\n",
    "  - **Initial conditions**: Does it match initial state (for time-dependent PDEs)?\n",
    "- Meshfree, flexible, can handle complex geometries\n",
    "\n",
    "**Key Advantage:** Automatic differentiation provides exact derivatives for free!\n",
    "\n",
    "### The PINN Training Process\n",
    "\n",
    "```\n",
    "1. Sample collocation points (x, y) in the domain\n",
    "2. Forward pass: u = NN(x, y)\n",
    "3. Compute derivatives: âˆ‚u/âˆ‚x, âˆ‚Â²u/âˆ‚xÂ², etc. (via autograd)\n",
    "4. Evaluate PDE residual: N[u] = 0\n",
    "5. Compute loss: L = w_pde * ||N[u]||Â² + w_bc * ||u - u_bc||Â²\n",
    "6. Backpropagate and update weights\n",
    "7. Repeat until convergence\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. The Poisson Equation\n",
    "\n",
    "### Problem Statement\n",
    "\n",
    "The **2D Poisson equation** is an elliptic PDE that appears in many physics problems:\n",
    "\n",
    "$$\\nabla^2 u = f(x, y) \\quad \\text{on } \\Omega = [0,1]^2$$\n",
    "\n",
    "with boundary conditions:\n",
    "\n",
    "$$u(x, y) = g(x, y) \\quad \\text{on } \\partial\\Omega$$\n",
    "\n",
    "where:\n",
    "- $\\nabla^2 u = \\frac{\\partial^2 u}{\\partial x^2} + \\frac{\\partial^2 u}{\\partial y^2}$ is the Laplacian\n",
    "- $f(x, y)$ is the source term\n",
    "- $g(x, y)$ specifies boundary values\n",
    "\n",
    "### Our Test Case\n",
    "\n",
    "We'll use a **manufactured solution** approach:\n",
    "\n",
    "**Analytical solution:** $u(x, y) = \\sin(\\pi x) \\sin(\\pi y)$\n",
    "\n",
    "**Source term:** $f(x, y) = -2\\pi^2 \\sin(\\pi x) \\sin(\\pi y)$\n",
    "\n",
    "**Boundary conditions:** $u = 0$ on all boundaries (Dirichlet)\n",
    "\n",
    "This allows us to:\n",
    "1. Verify PDE is satisfied: $\\nabla^2 u = f$\n",
    "2. Compute exact error: $||u_{PINN} - u_{analytical}||$\n",
    "3. Validate our implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Setup and Imports\n",
    "\n",
    "First, let's import the necessary libraries and check our environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard imports\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Add project root to path (adjust if needed)\n",
    "# If running from notebooks/, need to go up one level\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)\n",
    "\n",
    "# Scientific computing\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "# Our PINN modules\n",
    "from src.models import MLP\n",
    "from src.problems import PoissonProblem\n",
    "from src.training import train_pinn, PINNTrainer\n",
    "from src.interpretability import extract_activations_from_model\n",
    "\n",
    "# Check PyTorch version and device\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"\\nâœ“ All imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Creating the PINN Model\n",
    "\n",
    "### MLP Architecture\n",
    "\n",
    "Our PINN is a standard **Multi-Layer Perceptron (MLP)** with:\n",
    "- **Input**: $(x, y)$ coordinates (2D)\n",
    "- **Hidden layers**: 4 layers Ã— 64 neurons each\n",
    "- **Activation**: $\\tanh$ (smooth, bounded)\n",
    "- **Output**: Predicted solution $u(x, y)$ (1D)\n",
    "\n",
    "The architecture looks like:\n",
    "```\n",
    "Input (2) â†’ Linear(64) â†’ tanh â†’ Linear(64) â†’ tanh â†’ Linear(64) â†’ tanh â†’ Linear(64) â†’ tanh â†’ Linear(1)\n",
    "```\n",
    "\n",
    "**Why tanh?**\n",
    "- Smooth and differentiable (important for computing derivatives)\n",
    "- Bounded output helps with training stability\n",
    "- Commonly used in PINNs literature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the MLP model\n",
    "model = MLP(\n",
    "    input_dim=2,                    # (x, y) coordinates\n",
    "    hidden_dims=[64, 64, 64, 64],   # 4 hidden layers, 64 neurons each\n",
    "    output_dim=1,                   # Scalar solution u(x, y)\n",
    "    activation=\"tanh\"               # Smooth activation function\n",
    ")\n",
    "\n",
    "# Print model summary\n",
    "print(model)\n",
    "print(f\"\\nTotal parameters: {model.get_parameters_count():,}\")\n",
    "\n",
    "# Verify activation extraction works (for interpretability)\n",
    "test_input = torch.randn(10, 2)  # Batch of 10 points\n",
    "test_output = model(test_input)\n",
    "activations = model.get_activations()\n",
    "\n",
    "print(f\"\\nActivation extraction test:\")\n",
    "print(f\"  Input shape: {test_input.shape}\")\n",
    "print(f\"  Output shape: {test_output.shape}\")\n",
    "print(f\"  Captured activations: {list(activations.keys())}\")\n",
    "print(f\"  Layer 0 shape: {activations['layer_0'].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Defining the Problem\n",
    "\n",
    "Now let's create the `PoissonProblem` instance, which provides:\n",
    "- Analytical solution for validation\n",
    "- Source term $f(x, y)$\n",
    "- Boundary conditions\n",
    "- Collocation point sampling\n",
    "- PDE residual computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create problem instance\n",
    "problem = PoissonProblem(\n",
    "    domain_bounds=[(0.0, 1.0), (0.0, 1.0)]  # [0,1] Ã— [0,1] square domain\n",
    ")\n",
    "\n",
    "print(f\"Problem: {problem}\")\n",
    "print(f\"Domain: {problem.domain_bounds}\")\n",
    "print(f\"Input dimension: {problem.input_dim}\")\n",
    "print(f\"Output dimension: {problem.output_dim}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the Analytical Solution\n",
    "\n",
    "Let's visualize the true solution $u(x,y) = \\sin(\\pi x)\\sin(\\pi y)$ that we're trying to learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a grid for visualization\n",
    "x = np.linspace(0, 1, 100)\n",
    "y = np.linspace(0, 1, 100)\n",
    "X, Y = np.meshgrid(x, y)\n",
    "\n",
    "# Flatten grid to evaluate analytical solution\n",
    "xy_grid = torch.tensor(np.stack([X.flatten(), Y.flatten()], axis=1), dtype=torch.float32)\n",
    "u_analytical = problem.analytical_solution(xy_grid).detach().numpy().reshape(100, 100)\n",
    "\n",
    "# Plot\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Heatmap\n",
    "im = axes[0].pcolormesh(X, Y, u_analytical, cmap='viridis', shading='auto')\n",
    "axes[0].set_xlabel('x')\n",
    "axes[0].set_ylabel('y')\n",
    "axes[0].set_title('Analytical Solution: u(x,y) = sin(Ï€x)sin(Ï€y)')\n",
    "axes[0].set_aspect('equal')\n",
    "plt.colorbar(im, ax=axes[0])\n",
    "\n",
    "# Cross-section at y=0.5\n",
    "axes[1].plot(x, u_analytical[50, :], 'b-', linewidth=2, label='y=0.5')\n",
    "axes[1].set_xlabel('x')\n",
    "axes[1].set_ylabel('u(x, 0.5)')\n",
    "axes[1].set_title('Cross-section at y=0.5')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Solution range: [{u_analytical.min():.4f}, {u_analytical.max():.4f}]\")\n",
    "print(f\"Solution at center (0.5, 0.5): {u_analytical[50, 50]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample Collocation Points\n",
    "\n",
    "Unlike traditional numerical methods that use a fixed grid, PINNs sample **collocation points** where we enforce the PDE.\n",
    "\n",
    "We use two sampling strategies:\n",
    "- **Interior points**: Latin Hypercube Sampling (better coverage than random)\n",
    "- **Boundary points**: Uniform sampling on domain edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample collocation points\n",
    "n_interior = 1000\n",
    "n_boundary = 50  # Per edge\n",
    "\n",
    "interior_points = problem.sample_interior_points(n_interior, method='lhs')\n",
    "boundary_points = problem.sample_boundary_points(n_boundary)\n",
    "\n",
    "print(f\"Sampled {interior_points.shape[0]} interior points\")\n",
    "print(f\"Sampled {boundary_points.shape[0]} boundary points\")\n",
    "\n",
    "# Visualize sampling\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.scatter(interior_points[:, 0], interior_points[:, 1], \n",
    "            c='blue', s=1, alpha=0.5, label='Interior (LHS)')\n",
    "plt.scatter(boundary_points[:, 0], boundary_points[:, 1], \n",
    "            c='red', s=10, alpha=0.8, label='Boundary')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.title('Collocation Points Sampling')\n",
    "plt.legend()\n",
    "plt.xlim(-0.05, 1.05)\n",
    "plt.ylim(-0.05, 1.05)\n",
    "plt.gca().set_aspect('equal')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Training the PINN\n",
    "\n",
    "### Training Configuration\n",
    "\n",
    "The loss function combines:\n",
    "\n",
    "$$L = w_{pde} \\cdot L_{pde} + w_{bc} \\cdot L_{bc}$$\n",
    "\n",
    "where:\n",
    "- $L_{pde} = \\frac{1}{N_{int}} \\sum_{i=1}^{N_{int}} |\\nabla^2 u(x_i, y_i) - f(x_i, y_i)|^2$ (PDE residual)\n",
    "- $L_{bc} = \\frac{1}{N_{bc}} \\sum_{i=1}^{N_{bc}} |u(x_i^{bc}, y_i^{bc}) - g(x_i^{bc}, y_i^{bc})|^2$ (boundary error)\n",
    "\n",
    "**Training parameters:**\n",
    "- **Optimizer**: Adam (lr=1e-3)\n",
    "- **Epochs**: 5,000 (quick training for tutorial)\n",
    "- **Collocation points**: Resampled every epoch (prevents overfitting to specific points)\n",
    "- **Validation**: Compute relative L2 error every 100 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training configuration\n",
    "config = {\n",
    "    # Optimizer settings\n",
    "    \"optimizer\": \"adam\",\n",
    "    \"lr\": 1e-3,\n",
    "    \n",
    "    # Training duration\n",
    "    \"n_epochs\": 5000,\n",
    "    \n",
    "    # Collocation points\n",
    "    \"n_interior\": 1000,\n",
    "    \"n_boundary\": 50,\n",
    "    \n",
    "    # Loss weights\n",
    "    \"loss_weights\": {\n",
    "        \"pde\": 1.0,   # PDE residual weight\n",
    "        \"bc\": 1.0,    # Boundary condition weight\n",
    "        \"ic\": 0.0,    # Initial condition (not used for Poisson)\n",
    "    },\n",
    "    \n",
    "    # Device\n",
    "    \"device\": device,\n",
    "    \n",
    "    # Sampling and validation\n",
    "    \"resample_every\": 1,      # Resample collocation points every epoch\n",
    "    \"validate_every\": 100,    # Compute validation error every 100 epochs\n",
    "    \"print_every\": 500,       # Print progress every 500 epochs\n",
    "}\n",
    "\n",
    "print(\"Training Configuration:\")\n",
    "print(f\"  Optimizer: {config['optimizer']} (lr={config['lr']})\")\n",
    "print(f\"  Epochs: {config['n_epochs']}\")\n",
    "print(f\"  Interior points: {config['n_interior']}\")\n",
    "print(f\"  Boundary points: {config['n_boundary'] * 4} ({config['n_boundary']} per edge)\")\n",
    "print(f\"  Device: {config['device']}\")\n",
    "print(f\"  Loss weights: PDE={config['loss_weights']['pde']}, BC={config['loss_weights']['bc']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Training\n",
    "\n",
    "Now let's train the PINN! This will take a few minutes (faster on GPU)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"Starting PINN Training\")\n",
    "print(\"=\" * 80)\n",
    "print()\n",
    "\n",
    "# Train the model\n",
    "trained_model, history = train_pinn(model, problem, config)\n",
    "\n",
    "print()\n",
    "print(\"=\" * 80)\n",
    "print(\"Training Complete!\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print summary statistics\n",
    "print(\"\\nTraining Summary:\")\n",
    "print(\"-\" * 40)\n",
    "print(f\"Initial loss:        {history['loss_total'][0]:.6f}\")\n",
    "print(f\"Final loss:          {history['loss_total'][-1]:.6f}\")\n",
    "print(f\"Loss reduction:      {(1 - history['loss_total'][-1] / history['loss_total'][0]) * 100:.2f}%\")\n",
    "print(f\"\\nInitial L2 error:    {history['relative_l2_error'][0]:.4f}%\")\n",
    "print(f\"Final L2 error:      {history['relative_l2_error'][-1]:.4f}%\")\n",
    "print(f\"Error reduction:     {(1 - history['relative_l2_error'][-1] / history['relative_l2_error'][0]) * 100:.2f}%\")\n",
    "\n",
    "if history['relative_l2_error'][-1] < 1.0:\n",
    "    print(\"\\nâœ“ Excellent! Achieved <1% relative L2 error\")\n",
    "elif history['relative_l2_error'][-1] < 5.0:\n",
    "    print(\"\\nâœ“ Good! Error <5% (increase epochs for better accuracy)\")\n",
    "else:\n",
    "    print(\"\\nâš  Error still high (try more epochs or more collocation points)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Visualizing Results\n",
    "\n",
    "Let's visualize the training history and compare the PINN solution to the analytical solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(18, 4))\n",
    "\n",
    "# Total loss (log scale)\n",
    "axes[0].semilogy(history['epoch'], history['loss_total'], 'b-', linewidth=2)\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Total Loss (log scale)')\n",
    "axes[0].set_title('Training Loss Convergence')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Loss decomposition\n",
    "axes[1].semilogy(history['epoch'], history['loss_pde'], 'r-', linewidth=2, label='PDE residual', alpha=0.7)\n",
    "axes[1].semilogy(history['epoch'], history['loss_bc'], 'g-', linewidth=2, label='Boundary', alpha=0.7)\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Loss Components (log scale)')\n",
    "axes[1].set_title('Loss Decomposition')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# Relative L2 error\n",
    "axes[2].plot(history['epoch'], history['relative_l2_error'], 'purple', linewidth=2)\n",
    "axes[2].axhline(y=1.0, color='red', linestyle='--', linewidth=1, label='1% target')\n",
    "axes[2].set_xlabel('Epoch')\n",
    "axes[2].set_ylabel('Relative L2 Error (%)')\n",
    "axes[2].set_title('Validation Error')\n",
    "axes[2].legend()\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution Comparison\n",
    "\n",
    "Compare PINN solution vs analytical solution on a dense grid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate PINN on dense grid\n",
    "trained_model.eval()\n",
    "with torch.no_grad():\n",
    "    u_pinn = trained_model(xy_grid).detach().numpy().reshape(100, 100)\n",
    "\n",
    "# Compute error\n",
    "error = np.abs(u_pinn - u_analytical)\n",
    "\n",
    "# Create comparison plot\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# PINN solution\n",
    "im1 = axes[0].pcolormesh(X, Y, u_pinn, cmap='viridis', shading='auto')\n",
    "axes[0].set_xlabel('x')\n",
    "axes[0].set_ylabel('y')\n",
    "axes[0].set_title('PINN Solution')\n",
    "axes[0].set_aspect('equal')\n",
    "plt.colorbar(im1, ax=axes[0])\n",
    "\n",
    "# Analytical solution\n",
    "im2 = axes[1].pcolormesh(X, Y, u_analytical, cmap='viridis', shading='auto')\n",
    "axes[1].set_xlabel('x')\n",
    "axes[1].set_ylabel('y')\n",
    "axes[1].set_title('Analytical Solution')\n",
    "axes[1].set_aspect('equal')\n",
    "plt.colorbar(im2, ax=axes[1])\n",
    "\n",
    "# Absolute error\n",
    "im3 = axes[2].pcolormesh(X, Y, error, cmap='Reds', shading='auto')\n",
    "axes[2].set_xlabel('x')\n",
    "axes[2].set_ylabel('y')\n",
    "axes[2].set_title('Absolute Error')\n",
    "axes[2].set_aspect('equal')\n",
    "plt.colorbar(im3, ax=axes[2])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nError Statistics:\")\n",
    "print(f\"  Mean absolute error: {error.mean():.6f}\")\n",
    "print(f\"  Max absolute error:  {error.max():.6f}\")\n",
    "print(f\"  Std of error:        {error.std():.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-Sections\n",
    "\n",
    "Examine 1D slices to see how well PINN matches the analytical solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(18, 4))\n",
    "\n",
    "# Horizontal slice at y=0.5\n",
    "axes[0].plot(x, u_analytical[50, :], 'b-', linewidth=2, label='Analytical', alpha=0.7)\n",
    "axes[0].plot(x, u_pinn[50, :], 'r--', linewidth=2, label='PINN', alpha=0.7)\n",
    "axes[0].set_xlabel('x')\n",
    "axes[0].set_ylabel('u(x, 0.5)')\n",
    "axes[0].set_title('Horizontal Slice (y=0.5)')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Vertical slice at x=0.5\n",
    "axes[1].plot(y, u_analytical[:, 50], 'b-', linewidth=2, label='Analytical', alpha=0.7)\n",
    "axes[1].plot(y, u_pinn[:, 50], 'r--', linewidth=2, label='PINN', alpha=0.7)\n",
    "axes[1].set_xlabel('y')\n",
    "axes[1].set_ylabel('u(0.5, y)')\n",
    "axes[1].set_title('Vertical Slice (x=0.5)')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# Diagonal slice\n",
    "diag_analytical = np.diag(u_analytical)\n",
    "diag_pinn = np.diag(u_pinn)\n",
    "axes[2].plot(x, diag_analytical, 'b-', linewidth=2, label='Analytical', alpha=0.7)\n",
    "axes[2].plot(x, diag_pinn, 'r--', linewidth=2, label='PINN', alpha=0.7)\n",
    "axes[2].set_xlabel('x=y')\n",
    "axes[2].set_ylabel('u(x, x)')\n",
    "axes[2].set_title('Diagonal Slice (x=y)')\n",
    "axes[2].legend()\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Extracting Activations\n",
    "\n",
    "Now let's extract the neural activations for interpretability analysis. This is a key step for mechanistic interpretability!\n",
    "\n",
    "We'll:\n",
    "1. Extract activations on a dense grid\n",
    "2. Save to HDF5 file for efficient access\n",
    "3. Visualize activation patterns of individual neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract activations on 100x100 grid\n",
    "print(\"Extracting activations on dense grid...\")\n",
    "\n",
    "activation_store = extract_activations_from_model(\n",
    "    model=trained_model,\n",
    "    domain_bounds=[(0, 1), (0, 1)],\n",
    "    grid_resolution=100,\n",
    "    save_path=\"../data/activations/tutorial_poisson.h5\",\n",
    "    batch_size=1000\n",
    ")\n",
    "\n",
    "print(\"\\nâœ“ Activation extraction complete!\")\n",
    "\n",
    "# Display metadata\n",
    "metadata = activation_store.get_metadata()\n",
    "print(\"\\nActivation Store Metadata:\")\n",
    "print(f\"  Grid resolution: {metadata['grid_resolution']}\")\n",
    "print(f\"  Total points: {metadata['n_points']}\")\n",
    "print(f\"  Input dimension: {metadata['input_dim']}\")\n",
    "print(f\"  Layers: {metadata['layer_names']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Neuron Activations\n",
    "\n",
    "Let's look at what individual neurons in the first layer have learned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load layer 0 activations\n",
    "layer_0_acts = activation_store.load_layer(\"layer_0\")  # Shape: (10000, 64)\n",
    "coords = activation_store.load_coordinates()           # Shape: (10000, 2)\n",
    "\n",
    "print(f\"Layer 0 activations shape: {layer_0_acts.shape}\")\n",
    "print(f\"Coordinates shape: {coords.shape}\")\n",
    "\n",
    "# Visualize 4 neurons\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "neuron_indices = [0, 5, 15, 31]  # Pick some interesting neurons\n",
    "\n",
    "for idx, neuron_idx in enumerate(neuron_indices):\n",
    "    # Get activations for this neuron\n",
    "    neuron_acts = layer_0_acts[:, neuron_idx].reshape(100, 100)\n",
    "    \n",
    "    # Plot\n",
    "    im = axes[idx].pcolormesh(X, Y, neuron_acts, cmap='RdBu_r', shading='auto')\n",
    "    axes[idx].set_xlabel('x')\n",
    "    axes[idx].set_ylabel('y')\n",
    "    axes[idx].set_title(f'Layer 0, Neuron {neuron_idx}\\n(mean={neuron_acts.mean():.3f}, std={neuron_acts.std():.3f})')\n",
    "    axes[idx].set_aspect('equal')\n",
    "    plt.colorbar(im, ax=axes[idx])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nðŸ’¡ Observation: Different neurons learn different spatial features!\")\n",
    "print(\"   - Some respond to gradients (horizontal, vertical, diagonal)\")\n",
    "print(\"   - Some activate in specific regions (corners, edges, center)\")\n",
    "print(\"   - This is the foundation for mechanistic interpretability!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layer Summary Visualization\n",
    "\n",
    "Get an overview of what an entire layer has learned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize 16 neurons from layer 0\n",
    "fig, axes = plt.subplots(4, 4, figsize=(16, 16))\n",
    "\n",
    "for i in range(16):\n",
    "    row = i // 4\n",
    "    col = i % 4\n",
    "    \n",
    "    neuron_acts = layer_0_acts[:, i].reshape(100, 100)\n",
    "    \n",
    "    im = axes[row, col].pcolormesh(X, Y, neuron_acts, cmap='RdBu_r', shading='auto')\n",
    "    axes[row, col].set_title(f'Neuron {i}', fontsize=10)\n",
    "    axes[row, col].set_aspect('equal')\n",
    "    axes[row, col].set_xticks([])\n",
    "    axes[row, col].set_yticks([])\n",
    "\n",
    "plt.suptitle('Layer 0: First 16 Neurons', fontsize=16, y=0.995)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nðŸ’¡ Each neuron specializes in detecting different spatial patterns!\")\n",
    "print(\"   This diversity is crucial for the network to represent complex solutions.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Summary and Next Steps\n",
    "\n",
    "### What We've Learned\n",
    "\n",
    "In this tutorial, we:\n",
    "\n",
    "âœ… **Understood PINNs**: How neural networks solve PDEs using physics-informed loss functions\n",
    "\n",
    "âœ… **Implemented the Poisson equation**: Defined a manufactured solution test case\n",
    "\n",
    "âœ… **Trained a PINN**: Used automatic differentiation to compute PDE residuals\n",
    "\n",
    "âœ… **Validated results**: Compared PINN solution vs analytical solution\n",
    "\n",
    "âœ… **Extracted activations**: Stored neural activations for interpretability analysis\n",
    "\n",
    "âœ… **Visualized neuron patterns**: Saw how different neurons learn different spatial features\n",
    "\n",
    "### Key Insights\n",
    "\n",
    "1. **Automatic differentiation is key**: PyTorch computes exact derivatives for free\n",
    "2. **Collocation point sampling matters**: Latin Hypercube provides better coverage\n",
    "3. **Loss decomposition helps**: Separating PDE and BC losses allows fine-tuning\n",
    "4. **Neurons specialize**: Different neurons learn different spatial patterns\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "Now that you understand the basics, you can:\n",
    "\n",
    "1. **Experiment with hyperparameters**:\n",
    "   - Try different network architectures (more layers, more neurons)\n",
    "   - Test different activation functions (relu, gelu, sin)\n",
    "   - Adjust loss weights\n",
    "\n",
    "2. **Solve different PDEs**:\n",
    "   - Time-dependent: Heat equation\n",
    "   - Nonlinear: Burgers equation\n",
    "   - Wave propagation: Helmholtz equation\n",
    "\n",
    "3. **Deep dive into interpretability**:\n",
    "   - Train probing classifiers to detect derivatives\n",
    "   - Perform activation patching experiments\n",
    "   - Identify computational circuits\n",
    "\n",
    "4. **Explore advanced architectures**:\n",
    "   - Modified Fourier Networks (better for high-frequency solutions)\n",
    "   - Attention-Enhanced PINNs (for multi-scale problems)\n",
    "\n",
    "### Resources\n",
    "\n",
    "- **Documentation**: See `README.md` and `CLAUDE.md`\n",
    "- **Demo scripts**: Check `demos/demo_*.py` files for more examples\n",
    "- **Tests**: Inspect `tests/` for usage patterns\n",
    "- **Next tutorial**: `02_heat_equation.ipynb` (coming soon!)\n",
    "\n",
    "---\n",
    "\n",
    "**Congratulations!** You've successfully trained your first PINN! ðŸŽ‰"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises (Optional)\n",
    "\n",
    "Try these challenges to deepen your understanding:\n",
    "\n",
    "### Exercise 1: Architecture Exploration\n",
    "Train PINNs with different architectures and compare:\n",
    "- Shallow (2 layers Ã— 128 neurons)\n",
    "- Deep (8 layers Ã— 32 neurons)\n",
    "- Wide (4 layers Ã— 128 neurons)\n",
    "\n",
    "**Question**: Which architecture achieves the best error? Why?\n",
    "\n",
    "### Exercise 2: Activation Function Comparison\n",
    "Train PINNs with different activation functions:\n",
    "- `tanh` (smooth, bounded)\n",
    "- `relu` (piecewise linear)\n",
    "- `sin` (periodic)\n",
    "\n",
    "**Question**: How does activation choice affect convergence speed and final error?\n",
    "\n",
    "### Exercise 3: Sampling Strategy\n",
    "Compare different sampling methods:\n",
    "- Latin Hypercube Sampling (LHS)\n",
    "- Uniform Random Sampling\n",
    "- Grid Sampling\n",
    "\n",
    "**Question**: Which provides fastest convergence? Why?\n",
    "\n",
    "### Exercise 4: Custom PDE\n",
    "Define and solve your own Poisson equation with a different manufactured solution:\n",
    "- Try: $u(x,y) = x^2 + y^2$\n",
    "- Derive the corresponding source term $f(x,y)$\n",
    "- Train a PINN and validate\n",
    "\n",
    "**Hint**: For $u = x^2 + y^2$, compute $\\nabla^2 u$ analytically to get $f$.\n",
    "\n",
    "### Exercise 5: Neuron Analysis\n",
    "Analyze what specific neurons have learned:\n",
    "- Identify neurons that respond to horizontal gradients\n",
    "- Find neurons that activate in specific regions\n",
    "- Correlate neuron patterns with solution features\n",
    "\n",
    "**Question**: Can you find a neuron that computes $\\partial u/\\partial x$?\n",
    "\n",
    "---\n",
    "\n",
    "Good luck! ðŸš€"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

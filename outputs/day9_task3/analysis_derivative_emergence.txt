================================================================================
DERIVATIVE INFORMATION EMERGENCE ANALYSIS
Days 9, Tasks 1-3: Probing Results Summary
================================================================================

1. OVERALL STATISTICS
--------------------------------------------------------------------------------
Average RÂ² across all layers:
  - First derivatives (âˆ‚u/âˆ‚x, âˆ‚u/âˆ‚y):     0.8250
  - Second derivatives (âˆ‚Â²u/âˆ‚xÂ², âˆ‚Â²u/âˆ‚yÂ²): 0.1524
  - Laplacian (âˆ‡Â²u):                       0.0202

Best performing derivatives (Layer 3):
  - âˆ‚u/âˆ‚x: RÂ² = 0.9124
  - âˆ‚u/âˆ‚y: RÂ² = 0.8926
  - âˆ‚Â²u/âˆ‚xÂ²: RÂ² = 0.4636
  - âˆ‚Â²u/âˆ‚yÂ²: RÂ² = 0.5012
  - âˆ‡Â²u: RÂ² = 0.3424

2. LAYER-BY-LAYER ANALYSIS
--------------------------------------------------------------------------------

layer_0:
  âˆ‚u/âˆ‚x       : RÂ² = 0.7845  ðŸŸ¡ PARTIAL
  âˆ‚u/âˆ‚y       : RÂ² = 0.7883  ðŸŸ¡ PARTIAL
  âˆ‚Â²u/âˆ‚xÂ²     : RÂ² = -0.1355  ðŸ”´ NEGATIVE
  âˆ‚Â²u/âˆ‚yÂ²     : RÂ² = -0.1048  ðŸ”´ NEGATIVE
  âˆ‡Â²u         : RÂ² = -0.4062  ðŸ”´ NEGATIVE

layer_1:
  âˆ‚u/âˆ‚x       : RÂ² = 0.7965  ðŸŸ¡ PARTIAL
  âˆ‚u/âˆ‚y       : RÂ² = 0.7959  ðŸŸ¡ PARTIAL
  âˆ‚Â²u/âˆ‚xÂ²     : RÂ² = 0.0567  ðŸŸ  WEAK
  âˆ‚Â²u/âˆ‚yÂ²     : RÂ² = 0.0422  ðŸŸ  WEAK
  âˆ‡Â²u         : RÂ² = -0.0431  ðŸ”´ NEGATIVE

layer_2:
  âˆ‚u/âˆ‚x       : RÂ² = 0.8219  ðŸŸ¡ PARTIAL
  âˆ‚u/âˆ‚y       : RÂ² = 0.8075  ðŸŸ¡ PARTIAL
  âˆ‚Â²u/âˆ‚xÂ²     : RÂ² = 0.2016  ðŸŸ  WEAK
  âˆ‚Â²u/âˆ‚yÂ²     : RÂ² = 0.1938  ðŸŸ  WEAK
  âˆ‡Â²u         : RÂ² = 0.1875  ðŸŸ  WEAK

layer_3:
  âˆ‚u/âˆ‚x       : RÂ² = 0.9124  ðŸŸ¢ EXPLICIT
  âˆ‚u/âˆ‚y       : RÂ² = 0.8926  ðŸŸ¢ EXPLICIT
  âˆ‚Â²u/âˆ‚xÂ²     : RÂ² = 0.4636  ðŸŸ  WEAK
  âˆ‚Â²u/âˆ‚yÂ²     : RÂ² = 0.5012  ðŸŸ¡ PARTIAL
  âˆ‡Â²u         : RÂ² = 0.3424  ðŸŸ  WEAK

3. KEY FINDINGS
--------------------------------------------------------------------------------

Finding 1: First derivatives are explicitly encoded
  - âˆ‚u/âˆ‚x in layer_3: RÂ² = 0.9124 (>0.85 â†’ explicit)
  - âˆ‚u/âˆ‚y in layer_3: RÂ² = 0.8926 (>0.85 â†’ explicit)
  - Interpretation: The PINN explicitly computes and stores first
    derivatives in its hidden layer activations.

Finding 2: Second derivatives are NOT explicitly encoded
  - âˆ‚Â²u/âˆ‚xÂ² in layer_3: RÂ² = 0.4636 (<0.85 â†’ partial)
  - âˆ‚Â²u/âˆ‚yÂ² in layer_3: RÂ² = 0.5012 (<0.85 â†’ partial)
  - Interpretation: Second derivatives are computed via autograd
    during training, not stored in activations.

Finding 3: Hierarchical derivative computation
  - Pattern: 1st derivatives > 2nd derivatives > Laplacian
  - This matches the expected computational hierarchy:
    u â†’ âˆ‚u/âˆ‚x â†’ âˆ‚Â²u/âˆ‚xÂ² â†’ âˆ‡Â²u

Finding 4: Gradual emergence across layers
  - First derivatives: 0.7864 (layer_0) â†’ 0.9025 (layer_3)
    Improvement: +0.1161
  - Second derivatives: -0.1201 (layer_0) â†’ 0.4824 (layer_3)
    Improvement: +0.6026

4. CONCLUSIONS
--------------------------------------------------------------------------------

âœ… Hypothesis 1: CONFIRMED
   Early layers develop circuits for computing local derivatives.
   Evidence: RÂ² increases from layer_0 to layer_3 for first derivatives.

âœ… Finding: Two-stage derivative computation
   Stage 1: Network explicitly encodes first derivatives in activations
   Stage 2: Second derivatives computed via autograd during training
   This is an efficient computational strategy discovered by the PINN!

================================================================================

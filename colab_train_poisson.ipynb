{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Poisson PINN - Day 4 Task 2\n",
    "\n",
    "This notebook trains a PINN to solve the 2D Poisson equation and achieve <1% relative L2 error.\n",
    "\n",
    "**Configuration:**\n",
    "- Model: 4 hidden layers, 64 neurons each, tanh activation\n",
    "- Optimizer: Adam (lr=1e-3)\n",
    "- Training: 20,000 epochs\n",
    "- Collocation points: 10,000 interior + 400 boundary\n",
    "- Target: Relative L2 error < 1%\n",
    "\n",
    "**Instructions:**\n",
    "1. Enable GPU: Runtime â†’ Change runtime type â†’ GPU\n",
    "2. Run all cells\n",
    "3. Download the trained model and results\n",
    "4. Continue with Task 3 locally"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Dict, Optional, Tuple, Callable\n",
    "import time\n",
    "import os\n",
    "\n",
    "# Check GPU availability\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define MLP Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    \"\"\"Multi-Layer Perceptron for PINN.\"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim: int, hidden_dims: list, output_dim: int, activation: str = \"tanh\"):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dims = hidden_dims\n",
    "        self.output_dim = output_dim\n",
    "        self.activation_name = activation\n",
    "        \n",
    "        # Build layers\n",
    "        dims = [input_dim] + hidden_dims + [output_dim]\n",
    "        self.layers = nn.ModuleList([\n",
    "            nn.Linear(dims[i], dims[i+1]) for i in range(len(dims)-1)\n",
    "        ])\n",
    "        \n",
    "        # Activation function\n",
    "        if activation == \"tanh\":\n",
    "            self.activation_fn = torch.tanh\n",
    "        elif activation == \"relu\":\n",
    "            self.activation_fn = torch.relu\n",
    "        elif activation == \"gelu\":\n",
    "            self.activation_fn = torch.nn.functional.gelu\n",
    "        elif activation == \"sin\":\n",
    "            self.activation_fn = torch.sin\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown activation: {activation}\")\n",
    "        \n",
    "        # Xavier initialization\n",
    "        self._initialize_weights()\n",
    "    \n",
    "    def _initialize_weights(self):\n",
    "        for layer in self.layers:\n",
    "            nn.init.xavier_normal_(layer.weight)\n",
    "            nn.init.zeros_(layer.bias)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        h = x\n",
    "        for layer in self.layers[:-1]:\n",
    "            h = layer(h)\n",
    "            h = self.activation_fn(h)\n",
    "        h = self.layers[-1](h)\n",
    "        return h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Poisson Problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PoissonProblem:\n",
    "    \"\"\"2D Poisson equation: âˆ‡Â²u = f on [0,1]Â².\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.spatial_dim = 2\n",
    "        self.domain = [[0.0, 1.0], [0.0, 1.0]]\n",
    "    \n",
    "    def analytical_solution(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"u(x,y) = sin(Ï€x)sin(Ï€y)\"\"\"\n",
    "        return torch.sin(np.pi * x[:, 0:1]) * torch.sin(np.pi * x[:, 1:2])\n",
    "    \n",
    "    def source_term(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"f(x,y) = -2Ï€Â²sin(Ï€x)sin(Ï€y)\"\"\"\n",
    "        return -2 * np.pi**2 * torch.sin(np.pi * x[:, 0:1]) * torch.sin(np.pi * x[:, 1:2])\n",
    "    \n",
    "    def boundary_condition(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Dirichlet BC: u = 0 on âˆ‚Î©\"\"\"\n",
    "        return torch.zeros((x.shape[0], 1), device=x.device)\n",
    "    \n",
    "    def pde_residual(self, u: torch.Tensor, x: torch.Tensor, \n",
    "                     du_dx: torch.Tensor, d2u_dx2: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"PDE residual: âˆ‡Â²u - f\"\"\"\n",
    "        laplacian = d2u_dx2[:, 0:1] + d2u_dx2[:, 1:2]\n",
    "        source = self.source_term(x)\n",
    "        return laplacian - source\n",
    "    \n",
    "    def sample_interior_points(self, n: int, random_seed: Optional[int] = None) -> torch.Tensor:\n",
    "        \"\"\"Sample interior points using Latin Hypercube.\"\"\"\n",
    "        if random_seed is not None:\n",
    "            np.random.seed(random_seed)\n",
    "        \n",
    "        from scipy.stats import qmc\n",
    "        sampler = qmc.LatinHypercube(d=2, seed=random_seed)\n",
    "        samples = sampler.random(n=n)\n",
    "        \n",
    "        # Scale to domain\n",
    "        x = samples[:, 0] * (self.domain[0][1] - self.domain[0][0]) + self.domain[0][0]\n",
    "        y = samples[:, 1] * (self.domain[1][1] - self.domain[1][0]) + self.domain[1][0]\n",
    "        \n",
    "        return torch.tensor(np.column_stack([x, y]), dtype=torch.float32)\n",
    "    \n",
    "    def sample_boundary_points(self, n_per_edge: int, \n",
    "                               random_seed: Optional[int] = None) -> torch.Tensor:\n",
    "        \"\"\"Sample boundary points uniformly on edges.\"\"\"\n",
    "        x_min, x_max = self.domain[0]\n",
    "        y_min, y_max = self.domain[1]\n",
    "        \n",
    "        x = torch.linspace(x_min, x_max, n_per_edge)\n",
    "        y = torch.linspace(y_min, y_max, n_per_edge)\n",
    "        \n",
    "        # Bottom, top, left, right edges\n",
    "        bottom = torch.stack([x, torch.full_like(x, y_min)], dim=1)\n",
    "        top = torch.stack([x, torch.full_like(x, y_max)], dim=1)\n",
    "        left = torch.stack([torch.full_like(y, x_min), y], dim=1)\n",
    "        right = torch.stack([torch.full_like(y, x_max), y], dim=1)\n",
    "        \n",
    "        return torch.cat([bottom, top, left, right], dim=0)\n",
    "    \n",
    "    def compute_relative_l2_error(self, model: nn.Module, n_test_points: int = 10000,\n",
    "                                   random_seed: Optional[int] = None) -> float:\n",
    "        \"\"\"Compute relative L2 error.\"\"\"\n",
    "        model.eval()\n",
    "        x_test = self.sample_interior_points(n_test_points, random_seed)\n",
    "        x_test = x_test.to(next(model.parameters()).device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            u_pred = model(x_test)\n",
    "            u_exact = self.analytical_solution(x_test)\n",
    "            \n",
    "            error = torch.mean((u_pred - u_exact) ** 2)\n",
    "            norm = torch.mean(u_exact ** 2)\n",
    "            relative_error = torch.sqrt(error / norm) * 100\n",
    "        \n",
    "        return relative_error.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Derivative Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_derivatives(u: torch.Tensor, x: torch.Tensor, order: int = 1) -> torch.Tensor:\n",
    "    \"\"\"Compute derivatives using automatic differentiation.\"\"\"\n",
    "    if order == 1:\n",
    "        # First-order derivatives\n",
    "        grads = []\n",
    "        for i in range(x.shape[1]):\n",
    "            grad = torch.autograd.grad(\n",
    "                outputs=u,\n",
    "                inputs=x,\n",
    "                grad_outputs=torch.ones_like(u),\n",
    "                create_graph=True,\n",
    "                retain_graph=True,\n",
    "            )[0][:, i:i+1]\n",
    "            grads.append(grad)\n",
    "        return torch.cat(grads, dim=1)\n",
    "    \n",
    "    elif order == 2:\n",
    "        # Second-order derivatives (diagonal of Hessian)\n",
    "        du_dx = compute_derivatives(u, x, order=1)\n",
    "        hessian_diag = []\n",
    "        for i in range(x.shape[1]):\n",
    "            d2u_dx2 = torch.autograd.grad(\n",
    "                outputs=du_dx[:, i:i+1],\n",
    "                inputs=x,\n",
    "                grad_outputs=torch.ones_like(du_dx[:, i:i+1]),\n",
    "                create_graph=True,\n",
    "                retain_graph=True,\n",
    "            )[0][:, i:i+1]\n",
    "            hessian_diag.append(d2u_dx2)\n",
    "        return torch.cat(hessian_diag, dim=1)\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(f\"Order {order} not supported\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_pinn(model, problem, device, n_epochs=20000, n_interior=10000, n_boundary=100,\n",
    "               lr=1e-3, validate_every=100, print_every=500):\n",
    "    \"\"\"Train PINN on Poisson equation.\"\"\"\n",
    "    \n",
    "    model = model.to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    \n",
    "    history = {\n",
    "        'loss_total': [],\n",
    "        'loss_pde': [],\n",
    "        'loss_bc': [],\n",
    "        'relative_l2_error': [],\n",
    "    }\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "    print(\"Starting PINN Training\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"Device: {device}\")\n",
    "    print(f\"Interior points: {n_interior}\")\n",
    "    print(f\"Boundary points: {n_boundary * 4}\")\n",
    "    print(f\"Epochs: {n_epochs}\")\n",
    "    print(\"=\"*80)\n",
    "    print()\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        model.train()\n",
    "        \n",
    "        # Sample collocation points\n",
    "        x_interior = problem.sample_interior_points(n_interior).to(device)\n",
    "        x_boundary = problem.sample_boundary_points(n_boundary).to(device)\n",
    "        \n",
    "        # Enable gradients for PDE residual\n",
    "        x_interior = x_interior.requires_grad_(True)\n",
    "        \n",
    "        # Forward pass - interior\n",
    "        u_interior = model(x_interior)\n",
    "        du_dx = compute_derivatives(u_interior, x_interior, order=1)\n",
    "        d2u_dx2 = compute_derivatives(u_interior, x_interior, order=2)\n",
    "        residual = problem.pde_residual(u_interior, x_interior, du_dx, d2u_dx2)\n",
    "        loss_pde = torch.mean(residual ** 2)\n",
    "        \n",
    "        # Forward pass - boundary\n",
    "        u_boundary = model(x_boundary)\n",
    "        bc_exact = problem.boundary_condition(x_boundary)\n",
    "        loss_bc = torch.mean((u_boundary - bc_exact) ** 2)\n",
    "        \n",
    "        # Total loss\n",
    "        loss_total = loss_pde + loss_bc\n",
    "        \n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss_total.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Log history\n",
    "        history['loss_total'].append(loss_total.item())\n",
    "        history['loss_pde'].append(loss_pde.item())\n",
    "        history['loss_bc'].append(loss_bc.item())\n",
    "        \n",
    "        # Validation\n",
    "        if epoch % validate_every == 0:\n",
    "            rel_error = problem.compute_relative_l2_error(model, n_test_points=5000)\n",
    "            history['relative_l2_error'].append(rel_error)\n",
    "        else:\n",
    "            rel_error = None\n",
    "        \n",
    "        # Print progress\n",
    "        if epoch % print_every == 0:\n",
    "            elapsed = time.time() - start_time\n",
    "            print(f\"Epoch {epoch:5d}/{n_epochs} | \"\n",
    "                  f\"Loss: {loss_total.item():.6f} \"\n",
    "                  f\"(PDE: {loss_pde.item():.6f}, BC: {loss_bc.item():.6f}) | \"\n",
    "                  f\"Time: {elapsed:.1f}s\", end=\"\")\n",
    "            if rel_error is not None:\n",
    "                print(f\" | L2 Error: {rel_error:.4f}%\")\n",
    "            else:\n",
    "                print()\n",
    "    \n",
    "    # Final validation\n",
    "    final_error = problem.compute_relative_l2_error(model, n_test_points=10000)\n",
    "    history['relative_l2_error'].append(final_error)\n",
    "    \n",
    "    elapsed = time.time() - start_time\n",
    "    print()\n",
    "    print(\"=\"*80)\n",
    "    print(\"Training Complete!\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"Total time: {elapsed:.2f}s ({elapsed/60:.2f} minutes)\")\n",
    "    print(f\"Final loss: {history['loss_total'][-1]:.6f}\")\n",
    "    print(f\"Final relative L2 error: {final_error:.4f}%\")\n",
    "    if final_error < 1.0:\n",
    "        print(\"âœ… SUCCESS: Achieved target error < 1%\")\n",
    "    else:\n",
    "        print(f\"âš ï¸  Target not reached. Current: {final_error:.4f}%\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model and problem\n",
    "model = MLP(input_dim=2, hidden_dims=[64, 64, 64, 64], output_dim=1, activation=\"tanh\")\n",
    "problem = PoissonProblem()\n",
    "\n",
    "print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "print(f\"Device: {device}\")\n",
    "print()\n",
    "\n",
    "# Train\n",
    "trained_model, history = train_pinn(\n",
    "    model=model,\n",
    "    problem=problem,\n",
    "    device=device,\n",
    "    n_epochs=20000,\n",
    "    n_interior=10000,\n",
    "    n_boundary=100,\n",
    "    lr=1e-3,\n",
    "    validate_every=100,\n",
    "    print_every=500,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Loss curves\n",
    "axes[0].plot(history['loss_total'], label='Total Loss', linewidth=2)\n",
    "axes[0].plot(history['loss_pde'], label='PDE Loss', alpha=0.7)\n",
    "axes[0].plot(history['loss_bc'], label='BC Loss', alpha=0.7)\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].set_yscale('log')\n",
    "axes[0].set_title('Training Loss Curves')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Relative L2 error\n",
    "validation_epochs = [i * 100 for i in range(len(history['relative_l2_error']))]\n",
    "axes[1].plot(validation_epochs, history['relative_l2_error'], marker='o', linewidth=2, markersize=3)\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Relative L2 Error (%)')\n",
    "axes[1].set_title('Validation Error')\n",
    "axes[1].axhline(y=1.0, color='r', linestyle='--', alpha=0.5, label='1% target')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('training_history.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Training history saved to: training_history.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate solution heatmap\n",
    "model.eval()\n",
    "n_points = 100\n",
    "\n",
    "x = torch.linspace(0, 1, n_points)\n",
    "y = torch.linspace(0, 1, n_points)\n",
    "X, Y = torch.meshgrid(x, y, indexing='ij')\n",
    "grid_points = torch.stack([X.flatten(), Y.flatten()], dim=1).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    u_pred = model(grid_points).cpu().numpy().reshape(n_points, n_points)\n",
    "    u_exact = problem.analytical_solution(grid_points.cpu()).numpy().reshape(n_points, n_points)\n",
    "\n",
    "error = np.abs(u_pred - u_exact)\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "X_np = X.numpy()\n",
    "Y_np = Y.numpy()\n",
    "\n",
    "# PINN Solution\n",
    "im1 = axes[0].contourf(X_np, Y_np, u_pred, levels=50, cmap='viridis')\n",
    "axes[0].set_title('PINN Solution', fontsize=12, fontweight='bold')\n",
    "axes[0].set_xlabel('x')\n",
    "axes[0].set_ylabel('y')\n",
    "axes[0].set_aspect('equal')\n",
    "plt.colorbar(im1, ax=axes[0], label='u(x, y)')\n",
    "\n",
    "# Analytical Solution\n",
    "im2 = axes[1].contourf(X_np, Y_np, u_exact, levels=50, cmap='viridis')\n",
    "axes[1].set_title('Analytical Solution', fontsize=12, fontweight='bold')\n",
    "axes[1].set_xlabel('x')\n",
    "axes[1].set_ylabel('y')\n",
    "axes[1].set_aspect('equal')\n",
    "plt.colorbar(im2, ax=axes[1], label='u(x, y)')\n",
    "\n",
    "# Absolute Error\n",
    "im3 = axes[2].contourf(X_np, Y_np, error, levels=50, cmap='hot')\n",
    "axes[2].set_title(f'Absolute Error\\n(Max: {error.max():.2e}, Mean: {error.mean():.2e})',\n",
    "                  fontsize=12, fontweight='bold')\n",
    "axes[2].set_xlabel('x')\n",
    "axes[2].set_ylabel('y')\n",
    "axes[2].set_aspect('equal')\n",
    "plt.colorbar(im3, ax=axes[2], label='|u_pred - u_exact|')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('solution_heatmap.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Solution heatmap saved to: solution_heatmap.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Model and Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save trained model\n",
    "torch.save({\n",
    "    'model_state_dict': trained_model.state_dict(),\n",
    "    'history': history,\n",
    "    'final_error': history['relative_l2_error'][-1],\n",
    "    'config': {\n",
    "        'architecture': 'MLP',\n",
    "        'hidden_dims': [64, 64, 64, 64],\n",
    "        'activation': 'tanh',\n",
    "        'n_epochs': 20000,\n",
    "        'n_interior': 10000,\n",
    "        'n_boundary': 100,\n",
    "        'learning_rate': 1e-3,\n",
    "    }\n",
    "}, 'poisson_pinn_trained.pt')\n",
    "\n",
    "print(\"âœ… Model saved to: poisson_pinn_trained.pt\")\n",
    "print(\"\\nðŸ“¦ Download these files:\")\n",
    "print(\"  1. poisson_pinn_trained.pt (trained model)\")\n",
    "print(\"  2. training_history.png (loss curves)\")\n",
    "print(\"  3. solution_heatmap.png (solution visualization)\")\n",
    "print(\"\\nðŸŽ¯ Next: Continue with Task 3 locally!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
